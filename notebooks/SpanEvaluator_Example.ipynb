{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43297142",
   "metadata": {},
   "source": [
    "# Span Evaluator Example\n",
    "This notebook demonstrates how to use the `SpanEvaluator` class to create spans from a DataFrame of tokens and evaluate predictions.  \n",
    "The evaluation is performed at the **span level** using token-level Intersection over Union (IoU), which allows for partial matches between predicted and annotated entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f693ba5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stanza and spacy_stanza are not installed\n",
      "Flair is not installed by default\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from presidio_evaluator.evaluation.span_evaluator import SpanEvaluator\n",
    "from presidio_evaluator.data_objects import Span"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d7e7e",
   "metadata": {},
   "source": [
    "## Example DataFrame\n",
    "Below is a sample DataFrame representing tokenized text.  \n",
    "Each row corresponds to a token, with columns for:\n",
    "- `sentence_id`: The sentence this token belongs to.\n",
    "- `token`: The text of the token.\n",
    "- `annotation`: The ground truth entity label for the token.\n",
    "- `prediction`: The predicted entity label for the token.\n",
    "- `start`: The character start index of the token in the sentence.\n",
    "\n",
    "This data is designed so that some annotation spans and prediction spans only partially overlap, which will result in evaluation metrics (precision, recall, F1) between 0 and 1.\n",
    "\n",
    "- In sentence 1, \"John Doe\" is annotated as a PERSON, but only \"John\" is predicted as PERSON.\n",
    "- \"New York City\" is annotated as LOCATION, but only \"New\" and \"City\" are predicted as LOCATION (not \"York\").\n",
    "- In sentence 2, \"Jane Smith\" is annotated as PERSON, but only \"Smith\" is predicted as PERSON.\n",
    "- \"Paris\" is correctly annotated and predicted as LOCATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8efb8eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token</th>\n",
       "      <th>annotation</th>\n",
       "      <th>prediction</th>\n",
       "      <th>is_entity_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hello</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Doe</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>went</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>New</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>York</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>City</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token annotation prediction  is_entity_start\n",
       "0            1  Hello          O          O            False\n",
       "1            1    Mr.          O          O            False\n",
       "2            1   John     PERSON     PERSON             True\n",
       "3            1    Doe     PERSON          O            False\n",
       "4            1   went          O          O            False\n",
       "5            1     to          O          O            False\n",
       "6            1    New   LOCATION   LOCATION             True\n",
       "7            1   York   LOCATION   LOCATION            False\n",
       "8            1   City   LOCATION          O            False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample DataFrame with partially matching annotations and predictions\n",
    "# Example: annotation error splits 'John Doe' into two PERSON spans, prediction is correct\n",
    "sample_data = [\n",
    "    {\"sentence_id\": 1, \"token\": \"Hello\", \"annotation\": \"O\", \"prediction\": \"O\", \"is_entity_start\": False},\n",
    "    {\"sentence_id\": 1, \"token\": \"Mr.\", \"annotation\": \"O\", \"prediction\": \"O\", \"is_entity_start\": False},\n",
    "    {\"sentence_id\": 1, \"token\": \"John\", \"annotation\": \"PERSON\", \"prediction\": \"PERSON\", \"is_entity_start\": True},\n",
    "    {\"sentence_id\": 1, \"token\": \"Doe\", \"annotation\": \"PERSON\", \"prediction\": \"O\", \"is_entity_start\": False},  # Should be False if contiguous\n",
    "    {\"sentence_id\": 1, \"token\": \"went\", \"annotation\": \"O\", \"prediction\": \"O\", \"is_entity_start\": False},\n",
    "    {\"sentence_id\": 1, \"token\": \"to\", \"annotation\": \"O\", \"prediction\": \"O\", \"is_entity_start\": False},\n",
    "    {\"sentence_id\": 1, \"token\": \"New\", \"annotation\": \"LOCATION\", \"prediction\": \"LOCATION\", \"is_entity_start\": True},\n",
    "    {\"sentence_id\": 1, \"token\": \"York\", \"annotation\": \"LOCATION\", \"prediction\": \"LOCATION\", \"is_entity_start\": False},\n",
    "    {\"sentence_id\": 1, \"token\": \"City\", \"annotation\": \"LOCATION\", \"prediction\": \"O\", \"is_entity_start\": False},\n",
    "]\n",
    "df = pd.DataFrame(sample_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2fda21",
   "metadata": {},
   "source": [
    "## Create Spans from Tokens\n",
    "The `SpanEvaluator` reconstructs entity spans from token-level labels.  \n",
    "Adjacent tokens with the same entity label are merged into a single span.  \n",
    "This is important for evaluating at the entity (span) level rather than the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bdfb24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation Spans:\n",
      "Span(type: PERSON, value: ['John', 'Doe'], char_span: [2: 4])\n",
      "Span(type: LOCATION, value: ['New', 'York', 'City'], char_span: [6: 9])\n",
      "\n",
      "Prediction Spans:\n",
      "Span(type: PERSON, value: ['John'], char_span: [2: 3])\n",
      "Span(type: LOCATION, value: ['New', 'York'], char_span: [6: 8])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SpanEvaluator\n",
    "span_evaluator = SpanEvaluator()\n",
    "\n",
    "# Create annotation spans\n",
    "annotation_spans = span_evaluator._create_spans(df, \"annotation\")\n",
    "\n",
    "# Create prediction spans\n",
    "prediction_spans = span_evaluator._create_spans(df, \"prediction\")\n",
    "\n",
    "# Display the created spans\n",
    "print(\"Annotation Spans:\")\n",
    "for span in annotation_spans:\n",
    "    print(span)\n",
    "\n",
    "print(\"\\nPrediction Spans:\")\n",
    "for span in prediction_spans:\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efee4559",
   "metadata": {},
   "source": [
    "## Evaluate Predictions\n",
    "The `evaluate` method compares the annotation spans and prediction spans using token-level IoU.  \n",
    "For each annotation span, it finds the best-matching prediction span of the same entity type.  \n",
    "If the IoU is above the threshold (default 0.5), it is counted as a true positive.  \n",
    "Otherwise, it is a false negative (missed entity), and unmatched predictions are counted as false positives.\n",
    "\n",
    "The method returns:\n",
    "- Overall precision, recall, and F1 score\n",
    "- Per-entity-type metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e915a966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.0,\n",
       " 'recall': 0.0,\n",
       " 'f_beta': 0.0,\n",
       " 'per_type': {'PERSON': {'precision': 0.0, 'recall': 0.0, 'f_beta': 0.0},\n",
       "  'LOCATION': {'precision': 0.0, 'recall': 0.0, 'f_beta': 0.0}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the predictions\n",
    "results = span_evaluator.evaluate(df)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce541c9b",
   "metadata": {},
   "source": [
    "## Pairwise Span IoU Table for Error Analysis\n",
    "You can also compute the IoU for **all pairs** of annotation and prediction spans using `span_pairwise_iou_df`.  \n",
    "This is useful for detailed analysis and debugging, as it shows how well each predicted span overlaps with each annotated span.\n",
    "You can inspect the actual span objects and their IoU values for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb8fcb3",
   "metadata": {},
   "source": [
    "You have now seen how to use the SpanEvaluator to create spans and evaluate predictions from a token-level DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab5c004",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_results = span_evaluator.span_pairwise_iou_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36e9b984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>annotation_span</th>\n",
       "      <th>prediction_span</th>\n",
       "      <th>ann_entity</th>\n",
       "      <th>ann_start</th>\n",
       "      <th>ann_end</th>\n",
       "      <th>pred_entity</th>\n",
       "      <th>pred_start</th>\n",
       "      <th>pred_end</th>\n",
       "      <th>iou</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Span(type: PERSON, value: ['John', 'Doe'], cha...</td>\n",
       "      <td>Span(type: PERSON, value: ['John'], char_span:...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Span(type: PERSON, value: ['John', 'Doe'], cha...</td>\n",
       "      <td>Span(type: LOCATION, value: ['New', 'York'], c...</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Span(type: LOCATION, value: ['New', 'York', 'C...</td>\n",
       "      <td>Span(type: PERSON, value: ['John'], char_span:...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Span(type: LOCATION, value: ['New', 'York', 'C...</td>\n",
       "      <td>Span(type: LOCATION, value: ['New', 'York'], c...</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>LOCATION</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id                                    annotation_span  \\\n",
       "0            1  Span(type: PERSON, value: ['John', 'Doe'], cha...   \n",
       "1            1  Span(type: PERSON, value: ['John', 'Doe'], cha...   \n",
       "2            1  Span(type: LOCATION, value: ['New', 'York', 'C...   \n",
       "3            1  Span(type: LOCATION, value: ['New', 'York', 'C...   \n",
       "\n",
       "                                     prediction_span ann_entity  ann_start  \\\n",
       "0  Span(type: PERSON, value: ['John'], char_span:...     PERSON          2   \n",
       "1  Span(type: LOCATION, value: ['New', 'York'], c...     PERSON          2   \n",
       "2  Span(type: PERSON, value: ['John'], char_span:...   LOCATION          6   \n",
       "3  Span(type: LOCATION, value: ['New', 'York'], c...   LOCATION          6   \n",
       "\n",
       "   ann_end pred_entity  pred_start  pred_end       iou  \n",
       "0        4      PERSON           2         3  0.500000  \n",
       "1        4    LOCATION           6         8  0.000000  \n",
       "2        9      PERSON           2         3  0.000000  \n",
       "3        9    LOCATION           6         8  0.666667  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa17bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Span(type: PERSON, value: ['John', 'Doe'], char_span: [2: 4]),\n",
       " Span(type: PERSON, value: ['John', 'Doe'], char_span: [2: 4]),\n",
       " Span(type: LOCATION, value: ['New', 'York', 'City'], char_span: [6: 9]),\n",
       " Span(type: LOCATION, value: ['New', 'York', 'City'], char_span: [6: 9])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_results[\"annotation_span\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a7d527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Span(type: PERSON, value: ['John'], char_span: [2: 3]),\n",
       " Span(type: LOCATION, value: ['New', 'York'], char_span: [6: 8]),\n",
       " Span(type: PERSON, value: ['John'], char_span: [2: 3]),\n",
       " Span(type: LOCATION, value: ['New', 'York'], char_span: [6: 8])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_results[\"prediction_span\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29f6ccae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Span(type: LOCATION, value: ['New', 'York', 'City'], char_span: [6: 9])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=3\n",
    "iou_results[\"annotation_span\"].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44be63ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Span(type: LOCATION, value: ['New', 'York'], char_span: [6: 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_results[\"prediction_span\"].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59bfc4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Span(type: LOCATION, value: ['New', 'York'], char_span: [6: 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou_results[\"prediction_span\"].iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce63c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04d8b864",
   "metadata": {},
   "source": [
    "### Example: Why Merging Spans is Important\n",
    "This example demonstrates a scenario where annotation errors cause an entity to be split into multiple spans, while the prediction is correct. Merging adjacent spans is necessary for fair evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a03f393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>token</th>\n",
       "      <th>annotation</th>\n",
       "      <th>prediction</th>\n",
       "      <th>is_entity_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Hello</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Mr.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>John</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Doe</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>went</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>home</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id  token annotation prediction  is_entity_start\n",
       "0            1  Hello          O          O            False\n",
       "1            1    Mr.          O          O            False\n",
       "2            1   John     PERSON     PERSON             True\n",
       "3            1    Doe     PERSON     PERSON             True\n",
       "4            1   went          O          O            False\n",
       "5            1   home          O          O            False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example: annotation error splits 'John Doe' into two PERSON spans, prediction is correct\n",
    "example_data = [\n",
    "    {\"sentence_id\": 1, \"token\": \"Hello\", \"annotation\": \"O\", \"prediction\": \"O\", \"is_entity_start\": False},\n",
    "    {\"sentence_id\": 1, \"token\": \"Mr.\", \"annotation\": \"O\", \"prediction\": \"O\", \"is_entity_start\": False},\n",
    "    {\"sentence_id\": 1, \"token\": \"John\", \"annotation\": \"PERSON\", \"prediction\": \"PERSON\", \"is_entity_start\": True},\n",
    "    {\"sentence_id\": 1, \"token\": \"Doe\", \"annotation\": \"PERSON\", \"prediction\": \"PERSON\", \"is_entity_start\": True},  # Should be False if contiguous\n",
    "    {\"sentence_id\": 1, \"token\": \"went\", \"annotation\": \"O\", \"prediction\": \"O\", \"is_entity_start\": False},\n",
    "    {\"sentence_id\": 1, \"token\": \"home\", \"annotation\": \"O\", \"prediction\": \"O\", \"is_entity_start\": False},\n",
    "]\n",
    "example_df = pd.DataFrame(example_data)\n",
    "example_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444b2bf",
   "metadata": {},
   "source": [
    "In this example, the annotation mistakenly splits 'John' and 'Doe' into two PERSON spans due to the `is_entity_start` flag. The prediction correctly identifies 'John Doe' as a single PERSON span. Merging adjacent spans in the annotation is necessary for a fair IoU comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55c43e",
   "metadata": {},
   "source": [
    "## BIO Scheme Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data_bio = [\n",
    "    {\"sentence_id\": 1, \"token\": \"Hello\", \"annotation\": \"O\", \"prediction\": \"O\"},\n",
    "    {\"sentence_id\": 1, \"token\": \"Mr.\", \"annotation\": \"O\", \"prediction\": \"O\"},\n",
    "    {\"sentence_id\": 1, \"token\": \"John\", \"annotation\": \"B-PERSON\", \"prediction\": \"B-PERSON\"},\n",
    "    {\"sentence_id\": 1, \"token\": \"Doe\", \"annotation\": \"I-PERSON\", \"prediction\": \"O\"},\n",
    "    {\"sentence_id\": 1, \"token\": \"went\", \"annotation\": \"O\", \"prediction\": \"O\"},\n",
    "    {\"sentence_id\": 1, \"token\": \"to\", \"annotation\": \"O\", \"prediction\": \"O\"},\n",
    "    {\"sentence_id\": 1, \"token\": \"New\", \"annotation\": \"B-LOCATION\", \"prediction\": \"B-LOCATION\"},\n",
    "    {\"sentence_id\": 1, \"token\": \"York\", \"annotation\": \"I-LOCATION\", \"prediction\": \"O\"},\n",
    "    {\"sentence_id\": 1, \"token\": \"City\", \"annotation\": \"I-LOCATION\", \"prediction\": \"B-LOCATION\"}\n",
    "]\n",
    "\n",
    "df_bio = pd.DataFrame(sample_data_bio)\n",
    "df_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6546d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SpanEvaluator with BIO schema\n",
    "span_evaluator_bio = SpanEvaluator(schema=\"BIO\")\n",
    "\n",
    "# Create and display spans\n",
    "annotation_spans = span_evaluator_bio._create_spans(df_bio, \"annotation\")\n",
    "prediction_spans = span_evaluator_bio._create_spans(df_bio, \"prediction\")\n",
    "\n",
    "print(\"Annotation Spans:\")\n",
    "for span in annotation_spans:\n",
    "    print(span)\n",
    "\n",
    "print(\"\\nPrediction Spans:\")\n",
    "for span in prediction_spans:\n",
    "    print(span)\n",
    "\n",
    "# Evaluate\n",
    "results = span_evaluator_bio.evaluate(df_bio)\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d5a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the predictions\n",
    "results = span_evaluator_bio.evaluate(df_bio)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f676b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_results = span_evaluator_bio.span_pairwise_iou_df(df_bio)\n",
    "iou_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1680e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_results[\"annotation_span\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5b3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_results[\"prediction_span\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f93e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "presidio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
